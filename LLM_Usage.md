Of course. Here is the LLM usage disclosure document.

***

# LLM Usage Disclosure (`LLM_Usage.md`)

This document details the prompts and interactions with the Large Language Model (LLM), **Google's Gemini**, used in the development of this project. The LLM served as an interactive tool for code generation, iterative debugging, and report writing.

---
## 1. Initial Understanding and Sanity Check

The initial phase focused on understanding the repository and verifying its core functionality.

* **Prompt**: `https://github.com/MCG-NJU/AMD undertsand this`
    * **Reliance**: Relied on the LLM's summary of the repository's purpose, key features, and performance metrics to quickly grasp the project's scope.

* **Prompt**: `give me python code to achive this Set up the official or community re‑implementation (or your minimal re‑implementation) and run one small sanity experiment (tiny subset or toy data) to verify the method behaves as claimed qualitatively (e.g., the model can overfit a tiny set; or a unit test shows a key loss decreases; or one official example reproduces expected output).`
    * **Reliance**: Used the generated Python script (`sanity_check.py`) as the primary code for the initial experiment. This included the model instantiation, dummy data creation, and a minimal training loop to verify that the loss decreased.

---
## 2. Fine-tuning on a New Dataset (KTH)

This phase involved adapting the repository's code for a new task.

* **Prompt**: `Run on a new dataset (not in the paper) — core requirement (25 pts) Choose a dataset or test scenario that the paper did not use... A small custom diagnostic set you create (≥50 examples, clearly described). now give code for this`
    * **Reliance**: Relied on the LLM's **idea** to use the KTH Action Recognition dataset as a suitable novel dataset. Relied entirely on the generated code for `prepare_kth.py` (to create annotation files) and the initial version of `finetune_kth.py` (for the main experiment).

---
## 3. Iterative Debugging

A significant portion of the interaction involved a collaborative debugging process to resolve environment and code issues.

* **Prompts**: A series of prompts consisting of pasting Python tracebacks for errors such as:
    * `ModuleNotFoundError: No module named 'modeling_finetune'`
    * `RuntimeError: ...expected input... to have 3 channels, but got 16 channels instead`
    * `ImportError: cannot import name 'DataAugmentationForVideoMAE' from 'datasets'`
    * `SyntaxError: unterminated string literal`
    * `KeyError: 'model'`
    * `FileNotFoundError: ...train.txt`
* **Reliance**: Heavily relied on the LLM's analysis of each traceback to identify the root cause (e.g., incorrect directory, tensor shape mismatch, library name conflicts, typos, incorrect file paths, unexpected checkpoint format). Relied on the successively corrected and refined versions of the `finetune_kth.py` script provided by the LLM after each error. The final, self-contained version of the script was a direct output of this process.

---
## 4. Report Generation

The final phase involved summarizing the project's findings.

* **Prompts**:
    * `give 3–5 sentence note on what “worked” vs. diverged.`
    * `Report: • Dataset provenance & license, why it is “new” relative to the paper...`
    * `finetune got 49.32% in 20 epoch give the result table with this data`
    * `give error analysis little short`
    * `qualitative examples + 3 failure modes with this`
    * `create README.md`
    * `create Data card (DATA.md)`
    * `Cite all sources you used (including blogs, code, LLMs) for above code`
    * `LLM usage disclosure (LLM_Usage.md)`
* **Reliance**: Relied entirely on the LLM-generated text for all sections of the final report, including the **Results Table**, **Error Analysis**, **README**, **Data Card**, **Citations**, and this **Usage Disclosure** document. The analysis and hypothesized failure modes were generated by the LLM based on the experimental context and the final low-accuracy result.
